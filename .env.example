# Azure AI Foundry Configuration
# Endpoint format: https://<model-name>.<region>.models.ai.azure.com/ (Serverless) 
# OR your Azure OpenAI endpoint: https://<resource>.openai.azure.com/openai/deployments/<deployment>/chat/completions?api-version=...
AZURE_INFERENCE_ENDPOINT=<AZURE_INFERENCE_ENDPOINT>
AZURE_INFERENCE_CREDENTIAL=<AZURE_INFERENCE_CREDENTIAL>

# Model Name (Optional, used if your endpoint supports multiple models)
LLM_MODEL=<Deployed LLM Model Name>
